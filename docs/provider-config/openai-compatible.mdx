---
title: "Local OpenAI-Compatible Servers"
description: "Learn how to configure Cline with local AI model servers that offer OpenAI-compatible APIs, such as LM Studio, text-generation-webui, and other local hosting solutions."
---

Cline supports local AI model servers that offer APIs compatible with the OpenAI API standard. This allows you to run models locally on your own hardware while maintaining complete privacy and control over your data. This includes:

-   **LM Studio** - User-friendly local model hosting with OpenAI-compatible API
-   **text-generation-webui (oobabooga)** - Popular local model hosting solution
-   **vLLM** - High-performance local inference server
-   **LocalAI** - Drop-in replacement for OpenAI API running locally
-   **llamacpp-python** - Python bindings for llama.cpp with API server
-   **Any other local server** that provides an OpenAI-compatible API endpoint

This approach ensures your conversations and code never leave your machine, providing complete privacy and offline functionality.

### Configuration

To use a local OpenAI-compatible server with Cline, configure these main settings:

1.  **Base URL:** The local endpoint where your model server is running (e.g., `http://localhost:1234/v1`)
2.  **API Key:** Usually not required for local servers, but some may use a placeholder key
3.  **Model ID:** The name of the model as recognized by your local server

You'll find these settings in the Cline settings panel (click the ⚙️ icon):

-   **API Provider:** Select "OpenAI Compatible"
-   **Base URL:** Enter your local server's API endpoint (typically starts with `http://localhost:`)
-   **API Key:** Leave empty or enter a placeholder if required by your server
-   **Model:** Enter the model name as it appears in your local server
-   **Model Configuration:** Customize parameters for your local setup:
    -   Max Output Tokens
    -   Context Window size
    -   Input/Output pricing (set to $0 for local models)

### Common Local Server Configurations

#### LM Studio
-   **Base URL:** `http://localhost:1234/v1`
-   **API Key:** Not required (leave empty)
-   **Model:** The model name as shown in LM Studio's server interface

#### text-generation-webui (oobabooga)
-   **Base URL:** `http://localhost:5000/v1` (default port)
-   **API Key:** Not required (leave empty)
-   **Model:** The loaded model name

#### LocalAI
-   **Base URL:** `http://localhost:8080/v1` (default port)
-   **API Key:** May require a placeholder key depending on configuration
-   **Model:** The model name as configured in LocalAI

### Privacy and Security Benefits

Using local OpenAI-compatible servers with Cline provides:

-   **Complete Privacy:** Your code and conversations never leave your machine
-   **No Internet Dependency:** Works completely offline
-   **No Usage Costs:** No per-token charges or subscription fees
-   **Full Control:** Choose your own models and parameters
-   **No Rate Limits:** Use as much as your hardware can handle

### Troubleshooting

-   **Connection Refused:** Ensure your local server is running and accessible at the configured URL
-   **Model Not Found:** Verify the model name matches exactly what your server expects
-   **Slow Responses:** Consider adjusting your model size or hardware configuration
-   **Port Conflicts:** Make sure the server port isn't blocked by firewall or used by another application
-   **API Format Issues:** Ensure your server properly implements the OpenAI-compatible API format

### Performance Tips

-   Use models appropriate for your hardware (RAM and GPU capabilities)
-   Consider quantized models for better performance on consumer hardware
-   Adjust context window sizes based on your use case and hardware limits
-   Monitor system resources when running larger models

By using local OpenAI-compatible servers, you maintain complete control over your AI interactions while leveraging the full power of Cline for coding assistance.
