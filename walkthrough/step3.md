# Run Powerful Local Models

**Use LM Studio, Ollama, or an OpenAI-compatible local endpoint. No built-in cloud providers.**

Cline Local focuses on privacy-first, local inference. Point the extension to your local server (LM Studio, Ollama, or a self-hosted OpenAI-compatible API). You control models, latency, and cost, without sending code to third-party clouds. Cloud providers like Anthropic, Google, and OpenAI are not integrated in this fork by design.

![Cline Models Demo](https://storage.googleapis.com/cline_public_images/docs/assets/clines-models-hifi-3_compress.webp)
